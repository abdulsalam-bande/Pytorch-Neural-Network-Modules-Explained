{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9fe42b5-119b-4bc1-be72-ba7b43c2cdc3",
   "metadata": {},
   "source": [
    "## Markov Reward Process Tutorial\n",
    "\n",
    "### Introduction\n",
    "\n",
    "A **Markov Reward Process (MRP)** is an extension of a **Markov Chain** where each state is associated with a **reward**. This allows us to not only track transitions between states but also to accumulate rewards along the way. This concept is foundational to **Reinforcement Learning**, where the goal is often to maximize the total accumulated reward.\n",
    "\n",
    "In an MRP, the transition from one state to another is still governed by the **Markov Property**, but now we also consider the immediate reward received when transitioning from one state to the next.\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "A **Markov Reward Process** can be defined by the tuple \\((S, P, R)\\), where:\n",
    "\n",
    "- \\(S\\) is the set of states.\n",
    "- \\(P\\) is the transition probability matrix.\n",
    "- \\(R(s)\\) is the reward received when in state \\(s\\).\n",
    "\n",
    "The goal is to find the **value function** for each state, which represents the expected cumulative reward starting from that state.\n",
    "\n",
    "### Markov Reward Process Equations:\n",
    "\n",
    "In an MRP, the **value function** \\(V(s)\\) for a state \\(s\\) is given by:\n",
    "\n",
    "$$\n",
    "V(s) = R(s) + \\gamma \\sum_{s'} P(s' \\mid s) V(s')\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(R(s)\\) is the immediate reward received in state \\(s\\).\n",
    "- \\(P(s' \\mid s)\\) is the transition probability from state \\(s\\) to state \\(s'\\).\n",
    "- \\(\\gamma\\) is the discount factor that controls how much future rewards are valued compared to immediate rewards.\n",
    "\n",
    "### Components of the Markov Reward Process\n",
    "\n",
    "1. **States**: These represent possible situations in the environment.\n",
    "2. **Transition Probabilities**: These represent the likelihood of moving from one state to another, just like in a Markov Chain.\n",
    "3. **Rewards**: Each state has an associated reward, which could be positive or negative.\n",
    "4. **Value Function**: This function estimates the total expected reward starting from a particular state.\n",
    "\n",
    "### Empirical Approach for Estimating Value Functions\n",
    "\n",
    "While the **Bellman equation** provides a theoretical way to compute the value function, in many real-world scenarios, the transition probabilities and rewards might not be explicitly known. Instead, we can use an **empirical approach** to estimate the value function by simulating multiple episodes and averaging the observed returns.\n",
    "\n",
    "This approach follows these steps:\n",
    "\n",
    "1. **Simulate multiple episodes** starting from a given state.\n",
    "2. **Follow the transitions** according to the stochastic policy of the environment.\n",
    "3. **Record the cumulative discounted rewards** obtained in each episode.\n",
    "4. **Average the total returns** over all episodes to obtain an empirical estimate of the value function.\n",
    "\n",
    "This method is particularly useful when dealing with complex environments where exact transition probabilities are unknown or difficult to compute.\n",
    "\n",
    "### Let's implement a **Markov Reward Process** with an Empirical Approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd1a6db-e6f5-48d0-92e3-81db86c7661c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical value function for each state: {0: 0.0, 1: 4.1833, 2: 9.0686, 3: 0.0}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class GridWorldMarkovRewardProcess:\n",
    "    def __init__(self, gamma=0.9):\n",
    "        self.states = [0, 1, 2, 3]\n",
    "        self.terminal_states = [0, 3]\n",
    "        self.rewards = {0: 3, 3: 10}\n",
    "        self.gamma = gamma\n",
    "        self.transition_matrix = {\n",
    "            1: [0.7, 0.1, 0.1, 0.1],\n",
    "            2: [0.1, 0.1, 0.1, 0.7]\n",
    "        }\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return state in self.terminal_states\n",
    "\n",
    "    def step(self, state):\n",
    "        if self.is_terminal(state):\n",
    "            return state, 0\n",
    "        next_state_probs = self.transition_matrix[state]\n",
    "        next_state = random.choices(self.states, next_state_probs)[0]\n",
    "        reward = self.rewards.get(next_state, 0)\n",
    "        return next_state, reward\n",
    "\n",
    "    def compute_empirical_value_function(self, state, episodes=100, max_steps=10):\n",
    "        total_returns = []\n",
    "        for _ in range(episodes):\n",
    "            current_state = state\n",
    "            total_reward = 0\n",
    "            gamma_power = 1\n",
    "            for _ in range(max_steps):\n",
    "                if self.is_terminal(current_state):\n",
    "                    break\n",
    "                next_state, reward = self.step(current_state)\n",
    "                total_reward += gamma_power * reward\n",
    "                gamma_power *= self.gamma\n",
    "                current_state = next_state\n",
    "            total_returns.append(total_reward)\n",
    "        return sum(total_returns) / len(total_returns) if total_returns else 0\n",
    "\n",
    "env = GridWorldMarkovRewardProcess(gamma=0.9)\n",
    "empirical_values = {s: env.compute_empirical_value_function(s) for s in env.states}\n",
    "print(\"Empirical value function for each state:\", empirical_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec0ad776-41b4-4623-8f05-ce745926e7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/abdulsalamyazid/.zshenv:.:1: no such file or directory: /Users/abdulsalamyazid/.cargo/env\n",
      "/Users/abdulsalamyazid/PycharmProjects/swifty/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
